# Direct Preference Optimization
name: fpo

# beta: Temperature parameter for the TDPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.
# alpha: Temperature parameter for the TDPO loss, used to adjust the impact of sequential kl divergence.
beta: 0.1
alpha: 0.5
gamma: 0.1
simpo: False
use_mse: True

trainer: TDPOKLTrainer

dataloader: PairedPreferenceDataLoader

use_reference_model: true
